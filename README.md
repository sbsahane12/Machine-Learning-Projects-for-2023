# [Machine-Learning-Projects-for-2023](https://github.com/sbsahane12/Machine-Learning-Projects-for-2023.git)

# [Project 1: House Price Prediction Using Advanced Machine Learning](https://github.com/sbsahane12/House-Price-Prediction-Using-Advanced-Machine-Learning.git)
This is a project I did for my masters research paper, where I build a House Price Prediction system for a House dataset.
## Install
This project requires Python 3.6 and the following libraries installed:
- [NumPy](http://www.numpy.org/)
- [Pandas](http://pandas.pydata.org)
- [matplotlib](http://matplotlib.org/)
- [scikit-learn](http://scikit-learn.org/stable/)
- [seaborn](https://seaborn.pydata.org/)

## Models trained on: 
1. Linear Regression Algorithm 
2. Logistic Regression Algorithm 
3. Decision Tree
4. SVM
5. Naïve Bayes
6. KNN
7. K-Means Clustering
8. Random Forest

# [Project 2: Air-quality-prediction Using Advanced Machine Learning](https://github.com/sbsahane12/Air-quality-prediction-Using-Advanced-Machine-Learning.git) 
As air pollution is a complex mixture of toxic components with considerable impact on humans, forecasting air pollution concentration emerges as a priority for improving life quality. So with the help of Python tools and some Machine Learning algorithms, we try to predict the air quality. 

## Aim :
### In this project we will be building an Air Quality Index Predictor with the help of Machine Learning Models and Auto ML library i.e. TPOT

## Install
This project requires Python 3.6 and the following libraries installed:
- [NumPy](http://www.numpy.org/)
- [Pandas](http://pandas.pydata.org)
- [matplotlib](http://matplotlib.org/)
- [scikit-learn](http://scikit-learn.org/stable/)
- [seaborn](https://seaborn.pydata.org/)

You will also need to have software installed to run and execute a [Jupyter Notebook](http://ipython.org/notebook.html)

If you do not have Python installed yet, it is highly recommended that you install the [Anaconda](http://continuum.io/downloads) distribution of Python, which already has the above packages and more included.


## Models trained on: 
1. Linear Regression
2. Xgboost Regressor
3. Random Forest Regressor

****And doing the Hyperameter tuning for the above****

# [Project  3: Ecom-Market-Mix-Modle Using ML](https://github.com/sbsahane12/Ecom-Market-Mix-Modle-Using-ML.git) 
To build a MMM for ElecKart Ontario based ecommerce company
## Problem Statement  
### Background  - Business Understanding   
ElecKart is an e-commerce firm based out of Ontario, Canada specialising in electronic products. Over the last year, they had spent a significant amount of money on marketing. Occasionally, they had also offered big-ticket promotions (similar to the Big Billion Day). They are about to create a marketing budget for the next year, which includes spending on commercials, online campaigns, and pricing & promotion strategies. The CFO feels that the money spent over the last 12 months on marketing was not sufficiently impactful, and, that they can either cut on the budget or reallocate it optimally across marketing levers to improve the revenue response.  
## Install
This project requires Python 3.6 and the following libraries installed:
- [NumPy](http://www.numpy.org/)
- [Pandas](http://pandas.pydata.org)
- [matplotlib](http://matplotlib.org/)
- [scikit-learn](http://scikit-learn.org/stable/)
- [seaborn](https://seaborn.pydata.org/)
## Data Understanding
You have to use the data from July 2015 to June 2016. The data consists of the following types of information:  
   
Order level data  
•	FSN ID: The unique identification of each SKU  
•	Order Date: Date on which the order was placed  
•	Order ID: The unique identification number of each order  
•	Order item ID: Suppose you order 2 different products under the same order, it generates 2 different order Item IDs under the same order ID; orders are tracked by the Order Item ID.  
•	GMV: Gross Merchandise Value or Revenue  
•	Units: Number of units of the specific product sold  
•	Order payment type: How the order was paid – prepaid or cash on delivery  
•	SLA: Number of days it typically takes to deliver the product  
•	Cust id: Unique identification of a customer  
•	Product MRP: Maximum retail price of the product  
•	Product procurement SLA: Time typically taken to procure the product Apart from this, the following information is also available:  
•	Monthly spend on various advertising channels  
•	Days when there was any special sale  
•	Monthly NPS score – this may work as a proxy to ‘voice of the customer’  
•	Stock Index of the company on a monthly basis   

## Data Preparation  
You have to create market mix models for three product subcategories  - camera accessory, home audio and gaming accessory. Also, the models have to be built at a weekly level for each of the sub-categories.  
## Models trained on: 
1. Linear Regression
2. Xgboost Regressor

# [Project 4:Telecom Churn Logistic Regression with PCA](https://github.com/sbsahane12/Telecom-Churn-Logistic-Regression-with-PCA.git)
## Telecom Churn: Logistic Regression with PCA
With 21 predictor variables, we need to predict whether a particular customer will switch to another telecom provider or not. In telecom terminology, customer attrition is referred to as 'churn'.
## Install
This project requires Python 3.6 and the following libraries installed:
- [NumPy](http://www.numpy.org/)
- [Pandas](http://pandas.pydata.org)
- [matplotlib](http://matplotlib.org/)
- [scikit-learn](http://scikit-learn.org/stable/)
- [seaborn](https://seaborn.pydata.org/)

## Models trained on: 
1. PCA Algorithm 
2. Logistic Regression Algorithm 

# [Project  5: Countries  Clustering  Assignment Using ML](https://github.com/sbsahane12/Countries-Clustering-Assignment-Using-ML.git) 
## Problem Statement
HELP International is an international humanitarian NGO that is committed to fighting poverty and  providing the people of backward countries with basic amenities and relief during the time of  disasters and natural calamities
After the recent funding programmes, they have been able to raise around $ 10 million. Now the  CEO of the NGO needs to decide how to use this money strategically and effectively.
The significant issues that come while making this decision are mostly related to choosing the
countries that are in the direst need of aid.
As a Data Scientist, we need to find the countries in direst need and help CEO of HELP  International in using the fund money to reach right countries
## Problem Approach
As we have the Data of countries like child mortality rate, GDP Per Capita, Income etc. , we can
use Clustering to segregate the countries into different groups
Steps :
Data Inspection – Missing Values if any, EDA
Outlier Analysis
Data Pre-processing
Finding Optimal number of Clusters
Modelling
KMeans Clustering
Hierarchical Clustering – Single and Complete Linkages
Listing down top 5 countries in need
## Install
This project requires Python 3.6 and the following libraries installed:
- [NumPy](http://www.numpy.org/)
- [Pandas](http://pandas.pydata.org)
- [matplotlib](http://matplotlib.org/)
- [scikit-learn](http://scikit-learn.org/stable/)
- [seaborn](https://seaborn.pydata.org/)
## Result
- From the list of poor countries we obtained, sorted the list on income, gdpp, child_mortality rate.
- Top 5 countries which are in direst need :
- Congo, Dem Rep
- Liberia
- Burundi
- Niger
- Central African Republic

## Models trained on: 
#### Built the model using “Euclidean distance” as metric and linkage type as “Single”
- Plotted the Dendrogram for single linkage, we won’t be able to observe good clusters in single linkage
- Built the model using Complete Linkage, we could clearly observe 3 clusters formed. Used Cut_tree with n_clusters = 3 to get the labels of the clusters formed





# [Project  6: FineTech App Using ML](https://github.com/sbsahane12/-FineTech-App-Using-ML.git) 
## Problem Statement
- Follow the “Directing Customers to Subscription Through Financial App Behavior Analysis Machine Learning End to End Project” step by step to get 4 Bonus.
- We are finding how much time the customer takes to get enrolled in the premium feature app after registration. For that subtract ‘fineTech_appData.first_open’ from ‘fineTech_appData.enrolled_date’ and set data type as timedelta64 in hours.
## Problem Approach
- 1)We saw the heatmap correlation matrix but this was not showing correlation clearly but you can easily understand which feature is how much correlated with ‘enrolled’ feature using the above barplot.     
- 2)The ‘numscreens’ and ‘minigame’ is strongly positively correlated with ‘enrolled’ feature than other feature. 
- 3)The ‘hour’, ‘age’ and ‘used_premium_feature’ are strongly negatively correlated with the ‘enrolled’ feature.
## Steps :
- Data Inspection – Missing Values if any, EDA
- Outlier Analysis
- Data Pre-processing
- Finding Optimal number of Clusters
- Modelling
- KMeans Clustering
- Hierarchical Clustering – Single and Complete Linkages
- Listing down top 5 countries in need
## Install
This project requires Python 3.6 and the following libraries installed:
- [NumPy](http://www.numpy.org/)
- [Pandas](http://pandas.pydata.org)
- [matplotlib](http://matplotlib.org/)
- [scikit-learn](http://scikit-learn.org/stable/)
- [seaborn](https://seaborn.pydata.org/)
- 
## Machine Learning Model Building
- The target variable is categorical type 0 and 1, so we have to use supervised classification algorithms.To build the best model, we have to train and test the dataset with multiple Machine Learning algorithms then we can find the best ML model. So let’s try.First, we import the required packages.





# [Project  7: Handwriting digit recognition using SVM](https://github.com/sbsahane12/Handwriting-digit-recognition-using-SVM.git) 
## Problem Statement
As people have different handwriting it is difficult for a computer or any device to understand those handwritings of different people. The handwriting recognition is the ability of a computer or a device to take input handwriting in the form of an image such as picture of handwritten text which is fed to the pattern recognition algorithm or a model
## Steps :
- Data Inspection – Missing Values if any, EDA
- Outlier Analysis
- Data Pre-processing
- Finding Optimal number of Clusters
- Modelling
## Install
This project requires Python 3.6 and the following libraries installed:
- [NumPy](http://www.numpy.org/)
- [Pandas](http://pandas.pydata.org)
- [matplotlib](http://matplotlib.org/)
- [scikit-learn](http://scikit-learn.org/stable/)
- [seaborn](https://seaborn.pydata.org/)
## Conclusion : 
- Therefore as we can see in this scenario the non Linear model provides a higher accuracy than the Linear model which are **94%** and **91%** respectively.  
- Therefore we can conclude that the problem is non linear in nature.



# [Project  8: Heart Deasis Prediction Using ML Alogorithm](https://github.com/sbsahane12/Machine-Learning-Projects-for-2023.git) 
## Problem Statement
We have a data which classified if patients have heart disease or not according to features in it. We will try to use this data to create a model which tries predict if a patient has this disease or not. We will use logistic regression (classification) algorithm.
## Steps :
- Data Inspection – Missing Values if any, EDA
- Outlier Analysis
- Data Pre-processing
- Finding Optimal number of Clusters
- Modelling
- 
Data contains:
* age - age in years 
* sex - (1 = male; 0 = female)
* cp - chest pain type
* trestbps - resting blood pressure (in mm Hg on admission to the hospital) 
* chol - serum cholestoral in mg/dl
* fbs - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) 
* restecg - resting electrocardiographic results
* thalach - maximum heart rate achieved
* exang - exercise induced angina (1 = yes; 0 = no) 
* oldpeak - ST depression induced by exercise relative to rest
* slope - the slope of the peak exercise ST segment 
* ca - number of major vessels (0-3) colored by flourosopy
* thal - 3 = normal; 6 = fixed defect; 7 = reversable defect  
* target - have disease or not (1=yes, 0=no)
* 
## Install
This project requires Python 3.6 and the following libraries installed:
- [NumPy](http://www.numpy.org/)
- [Pandas](http://pandas.pydata.org)
- [matplotlib](http://matplotlib.org/)
- [scikit-learn](http://scikit-learn.org/stable/)
- [seaborn](https://seaborn.pydata.org/)
## Conclusion : 
- Our models work fine but best of them are KNN and Random Forest with 88.52% of accuracy. Let's look their confusion matrixes.




# [Project 9:HR - Attrition Analytics -  Exploratory Analysis & Predictive Modeling Using Advanced Machine Learning](https://github.com/sbsahane12/HR-Attrition-Analytics-Exploratory-Analysis-Predictive-Modeling-Using-Advanced-ML)
## Aim :
> Human Resources are critical resources of any organiazation. Organizations spend huge amount of time and money to hire 
> and nuture their employees. It is a huge loss for companies if employees leave, especially the key resources. 
> So if HR can predict weather employees are at risk for leaving the company, it will allow them to identify the attrition 
> risks and help understand and provie necessary support to retain those employees or do preventive hiring to minimize the 
> impact to the orgranization.

## Install
This project requires Python 3.6 and the following libraries installed:
- [NumPy](http://www.numpy.org/)
- [Pandas](http://pandas.pydata.org)
- [matplotlib](http://matplotlib.org/)
- [scikit-learn](http://scikit-learn.org/stable/)
- [seaborn](https://seaborn.pydata.org/)

You will also need to have software installed to run and execute a [Jupyter Notebook](http://ipython.org/notebook.html)

If you do not have Python installed yet, it is highly recommended that you install the [Anaconda](http://continuum.io/downloads) distribution of Python, which already has the above packages and more included.

### DATA ATRRIBUTES

- satisfaction_level: Employee satisfaction level <br>
- last_evaluation: Last evaluation  <br>
- number_project: Number of projects  <br>
- average_montly_hours: Average monthly hours <br>
- time_spend_company: Time spent at the company <br>
- Work_accident: Whether they have had a work accident <br>
- promotion_last_5years: Whether they have had a promotion in the last 5 years <br>
- department: Department <br>
- salary: Salary <br>
- left: Whether the employee has left <br>
## Models trained on: 
1. Linear Regression
2. Logistic Regression Algorithm 




# [Project  10:Lead scoring case study analysis using ML](https://github.com/sbsahane12/Lead-scoring-case-study-analysis-using-ML.git) 
## Problem Statement  
### Background  - Business Understanding   
- Building Logistic regression model & assigning Lead Scores to the prospective candidates of X Education
## Problem description
- X Education is an online Education company which has Lead database, some of which got converted & some didn‘t
- The typical lead conversion rate is 30% which is expected to be maximized to atleast 80%
- Target is to identify the ‘Hot Leads' which have a high conversion rate.
- The 'Hot Leads’ to be identified by cutoff Lead Scores Lead scores to be assigned to each candidates based on probabilities calculated by Logistic regression model

This project requires Python 3.6 and the following libraries installed:
- [NumPy](http://www.numpy.org/)
- [Pandas](http://pandas.pydata.org)
- [matplotlib](http://matplotlib.org/)
- [scikit-learn](http://scikit-learn.org/stable/)
- [seaborn](https://seaborn.pydata.org/)
## Data Preparation  
- Data Inspection and Missing value treatment
- Dummy variable creation
- Logistic regression modelling
- Model Accuracy Check
- Model fit on testdata
- Conclusion
- Recommendations\

## Data Inspection and Missing value treatment
- Columns containing >70% missing data were dropped.
- ‘City’ column had ~40% missing values & was dropped
- In absence of any visible correlation with Activity & Profile, these columns were dropped too
- *Asymmetric Index columns were checked for any possible relation to impute missing values Other columns with possible imputations were handled appropriately

- Unique value columns : 
- Columns with only one type of unique values were dropped in absence of variability

- Imputation :
- High missing value containing columns were imputed with suitable values

## Model Building : 
- 15 Features were selected using RFE.
- Six Logistic regression models were built iteratively
- Final model was selected based on:
- p-values <0.05 for all variables, indicatingsignificance 2.VIF< 5, indicating absence of multicollinearity
- Model performancemeasures
- High values of Accuracy, Sensitivity & Specificity indicate good predictive powers of model.
- Low False positive rate indicates model’s ability to
- predict positive values accurately.


## Recommendations

- Toget more customers, XEducation must keep the lead score lower, starting at ‘0’. But to achieve target conversion of greater than 80%, it should keep the cut off at 30.
- Thus, in the model, data frame changed for cut off Lead Score to gauge the Conversion percentages w.r.t. actual converted.
- Lowering the lead score cut off reduces conversion %, but it increases
number of actual converted.
- Based on the man power availability with XEducation, it may decide to give weightage to conversion %oractual numbers.


# [Project 11: Student Mark Predictor Project Deployment Using Advanced Machine Learning](https://github.com/sbsahane12/Student-Mark-Predictor-Project-Deployment-Using-Advanced-Machine-Learning-.git)
## Install
This project requires Python 3.6 and the following libraries installed:
- [NumPy](http://www.numpy.org/)
- [Pandas](http://pandas.pydata.org)
- [matplotlib](http://matplotlib.org/)
- [scikit-learn](http://scikit-learn.org/stable/)
- [seaborn](https://seaborn.pydata.org/)

## Models trained on: 
1. Linear Regression Algorithm 


# [Cotton-Disease-Prediction🌿](https://github.com/sbsahane12/Cotton-Disease-Prediction-.git)
![img1](https://user-images.githubusercontent.com/37875797/117844621-75bf1a00-b29d-11eb-9c7a-47ef88e9f50a.jpg)

## Table of Content
  * [Demo](#demo)
  * [Overview](#overview)
  * [Aim](#aim)
  * [Dataset](#dataset)
  * [Modeling](#modeling)
  * [Directory Tree](#directory-tree)
  * [Deployment](#deployment)
  * [Technical Aspect](#technical-aspect)
  * [Future Scope](#future-scope)
  * [Technologies Used](#technologies-used)
  * [Credits](#credits)

## Demo
[https://user-images.githubusercontent.com/37875797/Cotton_Disese_Prediction.mp4](https://user-images.githubusercontent.com/106218600/217471586-472efacf-eb0f-4ca4-8f81-944a7cf42b46.mp4)

## 📌Overview

India is the largest producer of cotton in the world. The United States Department of Agriculture (USDA) pegs India’s cotton production at 29 million bales in the 2019-20 season as against 26 million bales the previous year. The latest figures mean that India is all set to surpass China, which has a projection of 27.75 million bales for the same season. However, despite these impressive numbers, the productivity per hectare is starkly low.The production of cotton in india reducing gradualy over year because of major cotton diseases which impact their production very much some common diseases like insect attack,charcol rot and many are making heavy impact over their plantation.Due to this many cotton cultivators farmer get a huge drop down in their production and income.The problem will be solved if the farmer get to know about the plants which are infected and diseased in early stages of their growth so that farmers can use pesticides and different medicinal equipments to sprinkles medicines over plants and save their crops from diseases in early stages of production.As this project will help the farmers to recognize the cotton plants which are Fresh and Diseased by simply uploading the pictures of the cotton plants on the web app.we can also use drone to capture realtime images and uploaded to web app.

## 🎯Aim
There is a need for a system which can automatically detect the diseases as it can bring revolution in monitoring large fields of crop and then plant leaves can be taken cure as soon as possible after detection of disease. 

## Dataset
Dataset link - https://www.kaggle.com/janmejaybhoi/cotton-disease-dataset

![Screenshot (189)](https://user-images.githubusercontent.com/37875797/117844104-0ba67500-b29d-11eb-8083-e6ae2b31b85c.png)

In this dataset we are provided with images that belong to 4 classes : diseased leaf , diseased plant , fresh leaf and fresh plant. The objective of this study is to create a CNN model to help us predict whether these image of the leaf/plant belong to the diseased category or the healthy category.

## Modeling
> Keras transfer learning models -https://keras.io/api/applications/.

For modeling purposes, we used only keras transfer learning models.
First, we have done a basic image augmentation technique for the training dataset.

![Screenshot (191)](https://user-images.githubusercontent.com/37875797/117935983-f1fb4100-b321-11eb-9d56-4829e7f28564.png)

For transfer learning, we used three models.In these models we trained our dataset on top custom layer of the models and set other layers to non-trainable.
```
+-------+-------------------+------------------+------------------+---------------+
| Sl.N0 |       Model       | Number of epochs | Total parameters |  val_accuracy |
+-------+-------------------+------------------+------------------+---------------+
|   1   |      Resnet50     |        20        |     23989124     |     0.611     |
+-------+-------------------+------------------+------------------+---------------+
|   2   |    Resnet152v2    |        20        |     58733060     |     0.959     |
+-------+-------------------+------------------+------------------+---------------+
|   3   |    Inceptionv3    |        20        |     22007588     |      0.95     |
+-------+-------------------+------------------+------------------+---------------+
```
### Inception - Loss and Accuracy #
![Screenshot (193)](https://user-images.githubusercontent.com/37875797/117940775-0a218f00-b327-11eb-9b30-bd90fb5a81ca.png)

we got above 95% accuracy using transfer learning models.Resnet152v2 produces the highest accuracy but for deployment purposes, we use the inceptionv3 network because of fewer parameters in the network as compared to resnet. so we get a result in low latency.
## Directory Tree
```
├── Model
│   ├── model_inception.h5
├── template	
│   ├── disease_leaf.html
│   ├── disease_pant.html
|   |── healthy_leaf.html
|   |── healthy_leaf.html
|   |── index.html
├── cotton_disease_predictioin.ipynb
├── app.py	
├── requirement.txt
|── procfile
  ```

## Deployment
An implementation of a cotton leaf disease prediction machine learning model in TensorFlow. The application operates via a web interface where one may upload an image of a cotton plant/leaf and retrieve the prediction of whether or not it is infected.The app implemented by pthon flask framework

### Run your Flask APP #

   - Install all the PyPI requirements.
   ```
   > pip3 install -r requirements.txt
   ```
 - Manually place the model file `model_inception.h5` in the application directory (optional).
 - Start the Flask server.
   ```
   > flask run
   ```
   Make sure that the `FLASK_APP` enviroment variable is either empty or set to `app.py`. You can manually do that by executing `export FLASK_APP=app.py`.
   
   The application will try to locate the model i. e. `model_inception.h5` in the current directory, and if it is not found, download it.
* Note - model_inception.h5 not present in github repository because of larger size.

## Technical Aspect

1) Training image dataset using transfer learning models.
2) Deployement of model using flask.

* For training purposes, we use tensorflow.keras library.
* For deployment, we use Flask.

## Future Scope
* Deploy the model in any of the Paas.
* Developing front end that can recommend solution(pesticides and natural remedies) to the diseases.
* Accessible to all the regional langauges in india.

## Technologies Used
![](https://forthebadge.com/images/badges/made-with-python.svg)
* Jupyter Notebook
* Tensorflow
* Keras
* Flask

## Credits
[Kaggle](https://www.kaggle.com/janmejaybhoi/cotton-disease-dataset) - For conducting problem and providing this wonderful dataset.
